🧪 Starting sanity checks for latent injection training pipeline...
Device: cuda
==================================================
Testing LatentEncoder...
✓ LatentEncoder test passed! Output shape: torch.Size([4, 256])
  Latent range: [-0.080, 0.075]
==================================================
Testing forward process...
✓ Forward process test passed!
  Input shape: torch.Size([4, 128])
  Masked tokens: 181/512 (35.4%)
==================================================
Testing ReasoningLatentGenerator...
Using device: cuda
Loading thinker model...
🔍 DEBUG - Thinker model dtype: torch.float16
🔍 DEBUG - Latent encoder dtype: torch.float32
🔍 DEBUG - Device: cuda
Using 4 sample questions:
  1. What is the solution to the equation 2x + 5 = 17? Show your work step by step.
  2. A train leaves New York at 9:00 AM traveling at 120 mph. Another train leaves Bo...
  3. Explain the process of photosynthesis and why it's important for life on Earth. ...
  4. A company's revenue increased from $2.5 million to $3.2 million over two years. ...
Tokenized input shape: torch.Size([4, 63])
🔍 DEBUG - generate_latents called with input_ids dtype: torch.int64, shape: torch.Size([4, 63])
🔍 DEBUG - Thinker output dtype: torch.float16
🔍 DEBUG - Thinker output shape: torch.Size([1, 36, 2560])
🔍 DEBUG - About to pass to latent encoder...
❌ ERROR in latent encoder: mat1 and mat2 must have the same dtype, but got Half and Float
🔍 DEBUG - First few encoder layer dtypes:
   Layer 0 (Linear): torch.float32
mat1 and mat2 must have the same dtype, but got Half and Float
⚠ ReasoningLatentGenerator test failed: mat1 and mat2 must have the same dtype, but got Half and Float
  This might be due to missing model files. Continuing with other tests...
==================================================
Testing FiLM-enabled LLaDA model...
Creating FiLM model...
Loading original model...
Transferring weights...
