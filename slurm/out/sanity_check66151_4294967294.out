üß™ Starting sanity checks for latent injection training pipeline...
Device: cuda
==================================================
Testing LatentEncoder...
‚úì LatentEncoder test passed! Output shape: torch.Size([4, 256])
  Latent range: [-0.080, 0.075]
==================================================
Testing forward process...
‚úì Forward process test passed!
  Input shape: torch.Size([4, 128])
  Masked tokens: 181/512 (35.4%)
==================================================
Testing ReasoningLatentGenerator...
Using device: cuda
Loading thinker model...
üîç DEBUG - Thinker model dtype: torch.float16
üîç DEBUG - Latent encoder dtype: torch.float16
üîç DEBUG - Device: cuda
Using 4 sample questions:
  1. What is the solution to the equation 2x + 5 = 17? Show your work step by step.
  2. A train leaves New York at 9:00 AM traveling at 120 mph. Another train leaves Bo...
  3. Explain the process of photosynthesis and why it's important for life on Earth. ...
  4. A company's revenue increased from $2.5 million to $3.2 million over two years. ...
Tokenized input shape: torch.Size([4, 63])
üîç DEBUG - generate_latents called with input_ids dtype: torch.int64, shape: torch.Size([4, 63])
üîç DEBUG - Thinker output dtype: torch.float16
üîç DEBUG - Thinker output shape: torch.Size([1, 36, 2560])
üîç DEBUG - About to pass to latent encoder...
üîç DEBUG - Latent encoder output dtype: torch.float16
üîç DEBUG - Latent encoder output shape: torch.Size([1, 256])
üîç DEBUG - Final output dtype: torch.float32
Expected (4, 256), got torch.Size([1, 256])
‚ö† ReasoningLatentGenerator test failed: Expected (4, 256), got torch.Size([1, 256])
  This might be due to missing model files. Continuing with other tests...
==================================================
Testing FiLM-enabled LLaDA model...
Creating FiLM model...
Loading original model...
Transferring weights...
Transferred: 291 layers
Skipped: 0 layers
New FiLM parameters: 69,738,496
‚ö† FiLM model test failed: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 50.62 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 23.34 GiB is allocated by PyTorch, and 40.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  This might be due to missing model files. Continuing with other tests...
==================================================
Testing full training pipeline...
Loading LLaDA model with FiLM adapters...
Creating FiLM model...
Loading original model...
Transferring weights...
Transferred: 291 layers
Skipped: 0 layers
New FiLM parameters: 69,738,496
‚ö† Full pipeline test failed: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 50.62 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 23.28 GiB is allocated by PyTorch, and 104.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  This might be due to missing model files. Check the individual component tests above.
==================================================
Testing gradient flow...
Loading LLaDA model with FiLM adapters...
Creating FiLM model...
Loading original model...
Transferring weights...
Transferred: 291 layers
Skipped: 0 layers
New FiLM parameters: 69,738,496
‚ö† Gradient flow test failed: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 50.62 MiB is free. Including non-PyTorch memory, this process has 23.62 GiB memory in use. Of the allocated memory 23.28 GiB is allocated by PyTorch, and 104.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
==================================================
üéâ Sanity checks completed!

If all tests passed (‚úì), your training pipeline is ready to use.
If some tests failed (‚ö†), check that:
  1. Model files exist in ./llada-8b/ and ./thinker/
  2. All dependencies are installed
  3. You have sufficient GPU memory
