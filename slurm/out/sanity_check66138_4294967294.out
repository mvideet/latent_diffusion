ðŸ§ª Starting sanity checks for latent injection training pipeline...
Device: cuda
==================================================
Testing LatentEncoder...
âœ“ LatentEncoder test passed! Output shape: torch.Size([4, 256])
  Latent range: [-0.080, 0.075]
==================================================
Testing forward process...
âœ“ Forward process test passed!
  Input shape: torch.Size([4, 128])
  Masked tokens: 181/512 (35.4%)
==================================================
Testing ReasoningLatentGenerator...
Loading thinker model...
âš  ReasoningLatentGenerator test failed: Incorrect path_or_model_id: './thinker'. Please provide either the path to a local folder or the repo_id of a model on the Hub.
  This might be due to missing model files. Continuing with other tests...
==================================================
Testing FiLM-enabled LLaDA model...
âš  FiLM model test failed: Incorrect path_or_model_id: './llada-8b'. Please provide either the path to a local folder or the repo_id of a model on the Hub.
  This might be due to missing model files. Continuing with other tests...
==================================================
Testing full training pipeline...
Loading LLaDA model with FiLM adapters...
âš  Full pipeline test failed: Incorrect path_or_model_id: './llada-8b'. Please provide either the path to a local folder or the repo_id of a model on the Hub.
  This might be due to missing model files. Check the individual component tests above.
==================================================
Testing gradient flow...
Loading LLaDA model with FiLM adapters...
âš  Gradient flow test failed: Incorrect path_or_model_id: './llada-8b'. Please provide either the path to a local folder or the repo_id of a model on the Hub.
==================================================
ðŸŽ‰ Sanity checks completed!

If all tests passed (âœ“), your training pipeline is ready to use.
If some tests failed (âš ), check that:
  1. Model files exist in ./llada-8b/ and ./thinker/
  2. All dependencies are installed
  3. You have sufficient GPU memory
